packageName: Machine Learning in Julia
packageDescription: A Julia package for general, composable and elegant machine learning at scale.

extraButton: 
  name: Tutorials
  link: /tutorials

sections:
  - name: Get Started with MLJ
    subtitle: The past is a tapestry of what lies ahead
  - name: Why MLJ?
    subtitle: Who... let the dogs... out. Woof. Woof, woof
  - name: MLJ Partners
    subtitle: There is a glimmer of light in the sky

tours:
  - name: Install the MLJ Pacakge
    code: |
      # Create a new environment (optional)
      using Pkg; 
      Pkg.activate("mlj-env", shared=true)

      # Install MLJ
      Pkg.add("MLJ")

      # Test Installation (optional)
      Pkg.test("MLJ")

  - name: Train Your First Model
    code: |
      # 1. Load the Model
      Tree = @iload DecisionTreeClassifier

      # 2. Create an instance of the model
      tree = Tree()

      # 3. Wrap the model and data in a machine
      mach = machine(tree, X, y)

      # 4. Fit the machine
      fit!(mach)

      # 5. Make predictions
      yhat = predict(mach, X)

  - name: Evaluate Your Model
    code: |
      # Cross-validation code would go here

  - name: Hyperparameter Tuning
    code: |
      # Hyperparameter tuning code would go here


features:
  - title: Matching models to tasks
    content: A Model Registry stores detailed metadata for [over 200 models](google.com) and documentation can be searched without loading model code. **[See Models](/machines)**
    code: |
      julia> X, y = @load_iris
      julia> models(matching(X, y))
      54-element Vector
       (name = AdaBoostClassifier, package_name = MLJScikitLearnInterface, ... )
       (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )
       (name = BaggingClassifier, package_name = MLJScikitLearnInterface, ... )
       â‹®

      julia> models("pca")
      (name = PCA, package_name = MultivariateStats, ... )
      (name = PCADetector, package_name = OutlierDetectionPython, ... )

  - title: Tuning as wrapper
    content: >
      For improved composability, and to mitigate data hygiene issues, an extensive number of
      **meta-algorithms** are implemented as **model wrappers**:
      <br><br>  
        - [Tuning](https://juliaai.github.io/MLJ.jl/dev/tuning_models/)
        - Control of [iterative](https://juliaai.github.io/MLJ.jl/dev/controlling_iterative_models/) models
        - Correction for [class imbalance](https://juliaai.github.io/MLJ.jl/dev/correcting_class_imbalance/)
        - [Target transforms](https://juliaai.github.io/MLJ.jl/dev/target_transformations/) / inverse transforms
        - [Thresholding](https://juliaai.github.io/MLJ.jl/dev/thresholding_probabilistic_predictors/) probabilistic predictors
        - Homogeneous model [ensembling](https://juliaai.github.io/MLJ.jl/dev/homogeneous_ensembles/)
        - [Recursive feature elimination](https://juliaai.github.io/FeatureSelection.jl/dev/)
        - Wolpert model [stacking](https://juliaai.github.io/MLJ.jl/dev/model_stacking/)
      <br><br>
      In this way, a model wrapped in a tuning strategy, for example, becomes a "self-tuning"
      model, with all data resampling (e.g., cross-validation) managed under the hood."
    code: |
      model = XGBoostRegressor()
      r1 = range(model, :max_depth, lower=3, upper=10)
      r2 = range(model, :gamma, lower=0, upper=10, scale=:log)
      tuned_model = TunedModel(model, range=[r1, r2], resampling=CV(), measure=l2)

      # optimise and retrain on all data:
      mach = machine(tuned_model, data) |> fit!

      predict(mach, Xnew) # prediction w/ optim. params
      report(mach).best_model # inspect optim. results

  - title: Neural Networks
    content: Ding ding.
    code: 

  - title: Language Models
    content: Cool
    code:


  - title: Massive Arts
    content: Cool
    code:



users:
  - /users/1.png
  - /users/2.png
  - /users/3.png
  - /users/4.png
  - /users/5.png
  - /users/6.png
